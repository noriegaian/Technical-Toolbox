{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be running a lot of regressions and related models in this course.  This cheat sheet will show you how to run a regression on the sales data tab of the Class 3 Problem set.  As you will see, aside from some initial challenges with figuring out the command’s syntax, running a regression is generally very easy.  Knowing what to run and what to do with the results – that is much more challenging.\n",
    "\n",
    "We will start this set of Cheat Sheet tasks by building a very simple linear regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Order_Size} = \\beta_0 + \\beta_1 \\text{Ad_Budget} + \\beta_2 \\text{Dist}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.\tStart a new project and load the sales data**\n",
    "\n",
    "This data is tab 2 in \"MMA 860 Assessing and Testing Data File v1.0\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "\n",
    "#Build the path for the data file\n",
    "data_path = osp.join(\n",
    "    osp.curdir,'Data','MMA 860 Assessing and Testing Data File v1.0.xlsx')\n",
    "\n",
    "#Use the read_excel function to pull data from the 'Sales Data' sheet\n",
    "data = pd.read_excel(\n",
    "    data_path,sheet_name='Sales Data',index_col='Observation')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Run some summary statistics**\n",
    "\n",
    "As before, running $\\text{dtypes}$ and $\\text{describe()}$ is a good way to get a summary of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Look at the linear regression help file**\n",
    "\n",
    "To create our model, we will use the sci-kit learn library. Referencing the following documentation for linear regression, we can train our model.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required package from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "'''\n",
    "In order to input data from a pandas dataframe and into a sci-kit function,\n",
    "we need to convert the dataframe series into NumPy Arrays. This can be done\n",
    "with the values function.\n",
    "'''\n",
    "train_X = data[['Ad_Budget','Distance']].values\n",
    "train_y = data['Order_Size'].values\n",
    "\n",
    "'''\n",
    "Fitting data to a regression model requires two arguments, the training X\n",
    "values (independent variables) and the training y values (dependent variables.\n",
    "In general, most fit functions for models follow this format.\n",
    "'''\n",
    "reg = LinearRegression().fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now trained your first predictive model! Before proceeding, explore the link above to look at ways to assess your models performance. Additionally, test out the functions used to print the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think of model assessment as following this process:\n",
    "\n",
    "1. Does the model have any predictive power? (Check the F-Test)\n",
    "2. Do the variables we have included belong in the model? (Check the T-Tests)\n",
    "3. Have we violated any regression assumptions (Check the plots)\n",
    "4. Is the $R^2$ sufficient for business requirements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is an assessment of model fit – it represents the proportion of variability in $y$ explained by the model. The **Adjusted** $R^2$ adjusts for the number of variables included in a model, and is a better measure of model fit. The following script outputs the $R^2$ of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared:\", reg.score(train_X, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This linear regression model is defined by the three $\\beta$ coefficients seen above - $\\beta_0$ is sometimes called the intercept. The following script produces the coefficients for each variable, which are used to create predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computing intercept is trivial. The 'slope' coefficients are outputted as a\n",
    "tuple (consider this analogous to an array or list). We must index the tuple\n",
    "to get the numeric values. The values appear in the tuple in an order that \n",
    "corresponds to the position of the independent variable to which they are the\n",
    "slope of. For example, X_train has Ad_Budget values first then Distance values \n",
    "which results in the order presented below.\n",
    "'''\n",
    "\n",
    "print(\"B_0 =\",reg.intercept_)\n",
    "\n",
    "#Ad_Budget Coefficient\n",
    "print(\"B_1 =\",reg.coef_[0])\n",
    "\n",
    "#Distance Coefficient\n",
    "print(\"B_2 =\",reg.coef_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients substitute for beta values in the regression equation. In this example, you would replace $\\beta_0$ with $26.89$, $\\beta_1$ with $0.0027$, and $\\beta_2$ with $.144$. Then, by plugging in the values of any observation, you can produce a point estimate. Using the equation we developed, what is the predicted order size if $\\text{Ad_Budget}$ is $1000$ and $\\text{Dist}$ is $50$? As a bonus, use $\\text{reg.predict()}$ to determine this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#The prediction function takes a Numpy array as an argument\n",
    "reg.predict(np.array([1000,50]).reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Joint F-Test** is a formal hypothesis test to assess whether a model has any predictive ability. It tests the null hypothesis that all coefficients are jointly equal to $0$.\n",
    "\n",
    "The **t-tests** are formal hypothesis tests for each variable in the model. Each test assesses whether the coefficient for an individual variable is equal to $0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression there is an alternative library that exists called Statsmodel that can be used to do model assessment - yes it is in the Anaconda Distribution. One big advantage to using this library is the great summary that you get which contains the T and F statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "'''\n",
    "Fitting a model involves passing two arguments to ols: the general formula as\n",
    "a string and the data set used. Remember that the formula's attributes must\n",
    "match the column names in the dataframe. Then the fit() function is run and\n",
    "then summary() can be applied to that model.\n",
    "'''\n",
    "model = ols('Order_Size ~ Ad_Budget + Distance',data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Regression Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will come across a measurement known as a **residual**. Residuals are the set of differences between our training data and our model's prediction.\n",
    "\n",
    "A big indicator in the efficacy of the regression model is whether or not the residuals follow a **Normal Distribution**. This would suggest that the errors are due to random error and not some systematic flaw in our model. There are a few plots we can use to check this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual vs Fitted Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data plotted here should have no pattern (e.g., parabola, two clusters, cone shaped) and be evenly distributed around $0$ on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals calculated by definition above.\n",
    "predicted_y = reg.predict(train_X)\n",
    "#Note we can perform element-wise subtraction between arrays like so\n",
    "residuals = train_y - predicted_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(predicted_y,residuals,s=2,c='black')\n",
    "\n",
    "#This line adds the dashed horizontal line\n",
    "plt.hlines(0,min(predicted_y),max(predicted_y),color='red',linestyles='dashed')\n",
    "\n",
    "plt.xlabel(\"Model Prediction\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionally look at how close the mean of the residuals is to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of Residuals:',residuals.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Q-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Q plot gives insight into **multivariate normality**. The plot should look similar to the one below – the further the points are from the diagonal line, the more likely there are normality problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "'''\n",
    "Boilerplate code for creating a Normal Q-Q plot. The first two lines declare \n",
    "a figure and a subplot. This is an alternate way to output plots which allows \n",
    "for more than one plot per output.\n",
    "'''\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "'''\n",
    "Scipy.Stats has a built-in function for generating this type of plot. This \n",
    "function takes three arguments: the measurement being checked (residuals), \n",
    "the distribution we are checking against (normal in this case), and the plot \n",
    "to plot it to.\n",
    "'''\n",
    "stats.probplot(residuals,dist='norm',plot=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale-Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this plot to check the assumptions of equal variance in errors (**homoscedasticity**). You want to see a horizontal line with points spread randomly on either side. If there is a clear cone shaped pattern, it is evidence of heteroskedasticity.\n",
    "\n",
    "This plot has the predicted values on the x axis and the square root of the absolute value of the standardized residuals on the y axis. When a set of variables become standardized this means that they are referred to by the number of standard deviations that they lie away from their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First we create an array of normalized residuals using a Scikit function\n",
    "scaler = StandardScaler().fit(residuals.reshape(-1,1))\n",
    "norm_residuals = scaler.transform(residuals.reshape(-1,1))\n",
    "\n",
    "# Plot and take the root and absolute values of the norms\n",
    "plt.scatter(predicted_y,np.sqrt(np.abs(norm_residuals)),c='black',s=2)\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Root of standardized residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last plot worth observing – the density plot. This is another plot used to check for **normality of errors**. You would like to see a largely normal distribution – any other distinct pattern is indicative of a violation in assumptions. You can view it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mean, std = norm.fit(residuals)\n",
    "\n",
    "# Plot the histogram.\n",
    "plt.hist(residuals, bins=13, edgecolor='black', density=True)\n",
    "\n",
    "# Generate a PDF based on the fitted distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, color='black')\n",
    "title = \"Fit results: mu = %.2f,  std = %.2f\" % (mean, std)\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this moment to play around with the number of bins in the histogram. Note some observations on what happens if you have too many or too little bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals vs. Leverage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cook’s Distance plot helps you identify observations with **high leverage** (i.e., those that greatly influence the regression results). In this plot, the x axis is leverage, the y axis is the Studentized residuals (this is similar to standardized residual) and the size of the points indicates the Cook's Distance. Such a plot can be generated with the $\\text{influence_plot}$ function from the **StatsModel** library. We use the previously trained model in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a similar fashion to how the QQ plot was built, this plot can be generated.\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
