{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the standard assumptions in regression models is that the error term is identically and independently distributed. One common violation of this assumption occurs when there is heteroskedasticity, which means that the error term has a variance that follows a pattern.\n",
    "\n",
    "Typically, the amount of variability in the error term either increases or decreases with one or more variables, though more complex functions are possible too. For example, the variability of a return on a stock portfolio might depend on the number of stocks in that portfolio.\n",
    "\n",
    "The possibility of heteroskedasticity should be considered in most contexts. It can be tested for using a variety of tests. If heteroskedasticity does exist, its presence does not bias results, but it does make estimates less efficient (i.e. accurate) and it invalidates standard inferences (i.e. hypothesis testing and confidence interval calculation). The latter issue can be addressed by using a simple correction called the HCCME; the former requires a more sophisticated fix called **Generalized Least Squares (GLS)** if the form of heteroskedasticity is known or Feasible Generalized Least Squares if it is not. Neither form of GLS are in scope for this course.\n",
    "\n",
    "This cheat sheet will focus on the testing for heteroskedasticity and correcting the problem of inferences in the presence of heteroskedasticity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data into Python and train the model\n",
    "\n",
    "The data can be found in the $\\text{MMA 860 Assessing and Testing Data File v1.0}$ spreadsheet on its own worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join(\n",
    "    osp.curdir,'Data',\"MMA 860 Assessing and Testing Data File v1.0.xlsx\")\n",
    "data = pd.read_excel(path,sheet_name='Heteroskedasticity',index_col = 'Obs')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[['X1','X2']].values\n",
    "y_train = data[['Y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting the Scale Location Plot\n",
    "\n",
    "Examine the plot of the error term. You should see a fairly obvious example of heteroskedasticity similar to that shown in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_train - reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# First we create an array of normalized residuals using a Scikit function\n",
    "scaler = StandardScaler().fit(residuals.reshape(-1,1))\n",
    "norm_residuals = scaler.transform(residuals.reshape(-1,1))\n",
    "\n",
    "# Plot and take the root and absolute values of the norms\n",
    "plt.scatter(reg.predict(X_train),norm_residuals,c='black',s=2)\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Standardized Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that as the predicted value of Y increases, the variance of the residual also increases. By producing a ‘residuals by regressor’ plot by you can gain more insight into where the heteroskedasticity originates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To turn the X_train into the proper shape, we use the transpose function\n",
    "on the array and index the correct vector of information.\n",
    "'''\n",
    "plt.scatter(X_train.transpose()[0],residuals,c='black',s=2)\n",
    "plt.title(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.transpose()[1],residuals,c='black',s=2)\n",
    "plt.title(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Breusch-Pagan Test \n",
    "\n",
    "There are several tests for heteroskedasticity, notably one by **Breusch-Pagan**. The Breusch-Pagan test is specifically for heteroskedasticity, however there exist other specification tests (notably, **White**) that test for broader specification errors. You can run the Breusch-Pagan test using the StatsModel library\n",
    "\n",
    "The results of the Breusch-Pagan test will be a tuple of 4 values. The ones we are concerned about are the p-values. If either are less than $0.05$ then the model is found to be heteroskedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant libraries and train the model\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "results = ols('Y ~ X1 + X2',data).fit()\n",
    "\n",
    "#Perform the Breuch-Pagan Test by running this line\n",
    "bp = het_breuschpagan(results.resid,results.model.exog)\n",
    "measures = (\n",
    "    'LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value')\n",
    "print(dict(zip(measures,bp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HCCME (Heteroskedasticity Consistent Covariance Matrix Estimator) Method\n",
    "\n",
    "If specification problems are detected, you can correct the inferences in the regression by using the HCCME method. This corrects the covariance matrix in the background to fix our hypothesis tests. There is a statsmodels function that allows us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below, we apply the mentioned function. It takes one argument, the HCCME\n",
    "version. Typically HC3 is preferred, but HC2 can be used if your data \n",
    "has greater than 250 observations.\n",
    "'''\n",
    "corrected_model = results.get_robustcov_results(cov_type = 'HC3')\n",
    "corrected_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces heteroskedasticity-consistent inferences and tests in the same way a regular regression does. It will also use the corrected covariance matrix for any test statements used on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
